<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/ico" href="head.ico">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-112406581-4', 'auto');
      ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->

    <title>Qiang Meng</title>
  </head>
  <body style="background-color: #f9f9f9">
    <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
      <tr>
	<td>
	  <table>
            <tr>
              <td width="75%" valign="middle">
		<name>Qiang Meng (孟强)</name> <foot style="font-size: 0.70em"></foot>
		<br>            
		<p></p>
		I am currently a senior engineer at 
		<a href="https://kargobot.ai/">KargoBot</a>
		and 
		<a href="https://web.didiglobal.com/">DiDi</a>, specializing in perception for autonomous driving. 
		My expertise lies in the realms of 3D object detection, occupancy prediction, and data fusion, among other areas.
		Prior to my current role, I worked as an algorithm engineer and sub-core team leader at 
		<a href="https://www.aibee.com/">Aibee</a>. 
		During my time at Aibee, I actively contributed to diverse computer vision projects, encompassing tasks such as face recognition, image retrieval, and car/person re-identification, <em>etc</em>.

	   <!-- I am currently an algorithm engineer at AiBee working on computer vision including face recognition, image retrieval, car/person re-identification, <em>etc.</em>  -->
		<!-- I am also a sub-core leader here (<em>i.e.,</em> tech leader) leading a group with 5 employees and 2 interns. -->
		<br><br>

		I completed my Master's degree in Industrial Engineering at the  <a href="http://www.washington.edu/">University of Washington, Seattle</a>. 
		Prior to that, I received my Bachelor's degree in Mechanical Engineering from the <a href="https://en.ustc.edu.cn/">University of Science and Technology of China</a>, with a GPA of 3.78/4.3 (Rank: 3/61).

		<p align="center">
		  <a href="mailto:irvingmeng@outlook.com">E-mail</a> &nbsp|&nbsp
		  <!-- <a href="cv/cv.pdf">Curriculum Vitae</a> &nbsp|&nbsp -->
		  Curriculum Vitae &nbsp|&nbsp
		  <a href="https://scholar.google.com/citations?user=LdCZhUIAAAAJ&hl=zh-CN&oi=sra">Publications</a>
		  &nbsp|&nbsp
		  <!-- <a href="http://www.linkedin.com/in/abhoi/"> LinkedIn </a> &nbsp|&nbsp -->
		  <a href="https://github.com/IrvingMeng"> Github </a>
		  <!-- <a href="https://www.twitter.com/amlaanb"> Twitter </a> -->
		  <!--<span style="background-color: #FFFF00"><a href="https://abhoi.github.io/blog/"><b> Blog<span>&#42;</span> </b></a></span>-->
		</p>
              </td>
              <td width="40%">
		<a href="#top"><img class="profile-pic" src="images/profile_pic_200.jpg"></a>
		<!-- <center>
		     <img src="images/logos.png" width=25%>
		</center> -->
	      </td>
	    </tr>
	  </table>
	  <br>



	  <table>
	    <tr>
	      <td>
		<heading> News </heading>
		<ul style="list-style: none;">
			<li class="indent"> <strong>07/2024:</strong> &nbsp&nbsp Our paper <em>"Towards Stable 3D Object Detection"</em> was accepted by ECCV 2024.</li>
			<!-- <li class="indent"> <strong>03/2024:</strong> &nbsp&nbsp Make public the paper <em>"Small, Versatile and Mighty: A Range-View Perception Framework"</em> on arXiv.</li> -->
			<li class="indent"> <strong>02/2023:</strong> &nbsp&nbsp Our paper <em>"Curricular Object Manipulation in LiDAR-based Object Detection"</em> was accepted by CVPR 2023.</li>
			<!-- <li class="indent"> <strong>08/2022:</strong> &nbsp&nbsp Make public the paper <em>"Towards Privacy-Preserving, Real-Time and Lossless Feature Matching"</em> on arXiv.</li> -->
			<li class="indent"> <strong>04/2022:</strong> &nbsp&nbsp Invited to give a talk at Beijing Jiaotong University. </li>
		  <li class="indent"> <strong>03/2022:</strong> &nbsp&nbsp Invited to give a talk in <a href="https://readpaper.com/activity/iclr2022">ICLR 直播分享会</a> organized by ReadPaper. </li>
		  <li class="indent"> <strong>01/2022:</strong> &nbsp&nbsp Our Paper <em>"Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters"</em> was accepted by ICLR 2022 as a <strong>SPOTLIGHT</strong> paper. </li>
		  <!-- <li class="indent"> <strong>01/2022:</strong> &nbsp&nbsp Make public the paper <em>"Basket-based Softmax"</em> on arXiv.  </li> -->
		  <li class="indent"> <strong>11/2021:</strong> &nbsp&nbsp Invited to give a talk in the <a href="https://eab.org/events/program/261"> workshop on face image quality</a> organized by <a href="https://eab.org/">EAB</a>, <a href="https://www.dhs.gov/">DHS-OBIM</a>, <a href="https://www.nist.gov/">NIST</a>, <a href="https://www.eulisa.europa.eu/">eu-LISA</a>, etc.
		    <!-- 欧洲生物特征识别协会(European Association for Biometrics, EAB),  美国国土安全部生物特征身份管理办公室(DHS Office of Biometric Identity Management)， 美国国家标准与技术研究院(National Institute of Standards and Technology, NIST)， 欧盟eu-LISA机构 -->
		  <li class="indent"> <strong>07/2021:</strong>   &nbsp&nbsp Our paper <em>"Learning Compatible Embeddings"</em> was accepted by ICCV 2021.</li>
		  <!-- <li class="indent"> <strong>07/2021:</strong>   &nbsp&nbsp Make public the paper <em>"PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition"</em> on arXiv.</li> -->
		  <li class="indent"> <strong>06/2021:</strong> &nbsp&nbsp Invited to give a talk about MagFace in <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/p_60c9c516e4b0f120ffc7b72b/6">CVPR 论文分享会</a> organized by 机器之心. </li>
		  <li class="indent"> <strong>03/2021:</strong>   &nbsp&nbsp Our paper <em>"MagFace: A Universal Representation for Face Recognition and Quality Assessment"</em> was accepted by CVPR 2021 as an <strong>ORAL</strong> paper.</li>
		  <li class="indent"> <strong>12/2020:</strong>   &nbsp&nbsp Our paper <em>"Searching for Alignment in Face Recognition"</em> was accepted by AAAI 2021.</li>
		  <!-- <li class="indent"> <strong>08/2018:</strong>   &nbsp&nbsp Got my master degree :), but quit the PhD program :(.</li> 	
		  <li class="indent"> <strong>06/2015:</strong>   &nbsp&nbsp Got my bachelor degree. </li> 	 -->
		</ul>
	      </td>
	    </tr>
	  </table>


	  <br>

	  <!-- To fix heading alignment -->
	  <table>
	    <tr>
	      <td><heading>Selected Publications</heading></td>
	    </tr>
	  </table>
	  My research interests lie in computer vision, deep learning and optimization.
	  Representative works are <span style="background-color: #FFFF00">highlighted</span>. 
	  <br>
	  <br>


	<table bgcolor="#ffffd0" cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/si.png'>
	    </td>
	      <td valign="top" width="75%">
		<!-- <a href="https://arxiv.org/abs/2403.00325"> -->
		  <papertitle>Towards Stable 3D Object Detection</papertitle>
		<!-- </a> -->
		<br>
		Jiabao Wang*, <strong>Qiang Meng*</strong>, Guochao Liu, Liujiang Yan,  Ke Wang, Mingming Cheng, Qibin Hou		
		<br>
		*equal contirbution <br>
		<em>ECCV</em>, 2024
		<br>
		<a href="https://jbwang1997.github.io/projects/stability_index/index.html">project page</a>
		| 
		paper 
		| 
		code
		<iframe src="https://ghbtns.com/github-btn.html?user=jbwang1997
		&repo=StabilityIndex&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
		<br> 
		This paper designs a novel stability index (SI) to evaluate the stability of 3D object detection models, and proposes a strong baseline for stability improvement.
	    </td>
	    </tr>
	  </table>	

	  <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/svm.png'>
	    </td>
	      <td valign="top" width="75%">
		<a href="https://arxiv.org/abs/2403.00325">
		  <papertitle>Small, Versatile and Mighty: A Range-View Perception Framework</papertitle>
		</a>
		<br>
		<strong>Qiang Meng</strong>, Xiao Wang, JiaBao Wang, Liujiang Yan, Ke Wang
		<br>
		<em>arXiv</em>, 2024
		<br>
		<a href="https://arxiv.org/pdf/2403.00325">paper</a>
		<br>
		The Small, Versatile, and Mighty (SVM) framework is a range-view-based perception system which can perform object detection, semantic segmentation and panoptic segmentation.
	    </td>
	    </tr>
	  </table>

	
	  <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/com.png'>
	    </td>
	      <td valign="top" width="75%">
		<a href="https://arxiv.org/abs/2304.04248">
		  <papertitle>Curricular Object Manipulation in LiDAR-based Object Detection</papertitle>
		</a>
		<br>
		Ziyue Zhu*, <strong>Qiang Meng*</strong>, Xiao Wang, Ke Wang, Liujiang Yan, Jian Yang
		<br>
		*equal contirbution <br>
		<em>CVPR</em>, 2023
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Curricular_Object_Manipulation_in_LiDAR-Based_Object_Detection_CVPR_2023_paper.pdf">paper</a>
		|
		<a href="https://github.com/ZZY816/COM">code</a> 
		<iframe src="https://ghbtns.com/github-btn.html?user=ZZY816&repo=COM&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
		<br>
		Curricular object manipulation (COM) is a framework for LiDAR-based object detection that incorporates the
		easy-to-hard training strategy into both loss design and
		augmentation process. 
	      </td>
	    </tr>
	  </table>

	  <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/securevector/1.png'>
	    </td>
	      <td valign="top" width="75%">
		<a href="http://arxiv.org/abs/2208.00214">
		  <papertitle>Towards Privacy-Preserving, Real-Time and Lossless Feature Matching</papertitle>
		</a>
		<br>
		<strong>Qiang Meng</strong>, Feng Zhou
		<br>
		<em>arXiv</em>, 2022
		<br>
		<a href="https://arxiv.org/pdf/2208.00214">paper</a>
		|
		<a href="https://github.com/IrvingMeng/SecureVector">code</a> 
		<iframe src="https://ghbtns.com/github-btn.html?user=IrvingMeng&repo=SecureVector&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
		<br>
		SecureVector is a plug-in module designed to accomplish real-time and lossless feature matching among sanitized features, offering significantly higher security levels compared to current state-of-the-art solutions.
	      </td>
	    </tr>
	  </table>

	  <table bgcolor="#ffffd0" cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/privacyface/1.png'>
	    </td>
	      <td valign="top" width="75%">
		<a href="https://irvingmeng.github.io/projects/privacyface">
		  <papertitle>Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters</papertitle>
		</a>
		<br>
		<strong>Qiang Meng</strong>, Feng Zhou, Hainan Ren, Tianshu Feng, Guochao Liu, Yuanqing Lin
		<!-- <a href="https://www.linkedin.com/in/ganeshjcs">test</a> -->
		<br>
		<em>ICLR </em>, 2022   <strong><font color="#FF0000">(Spotlight)</font> </strong>
		<br>
		<a href="https://irvingmeng.github.io/projects/privacyface">project page</a>
		|
		<a href="https://openreview.net/pdf?id=7l1IjZVddDW">paper</a>
		|
		<a href="https://zhuanlan.zhihu.com/p/475802585">知乎</a>
		<br>
		A pragmatic framework that markedly enhances the performance of federated learning in face recognition while ensuring privacy guarantees. Key components encompass a meticulously crafted differentially private local clustering mechanism and a recognition loss that is consensus-aware.

		<!-- We design a practical Differentially Private Local Clustering (DPLC) mechanism and a consensus-aware recognition loss -->
		<!-- https://zhuanlan.zhihu.com/p/393598428 -->
	      </td>
	    </tr>
	  </table>
	  
	  <table cellspacing="10">
	    <tr onmouseout="lce_stop()" onmouseover="lce_start()" >
	      <td width="25%">
		<div class="one">
		  <div class="two" id = 'lce_image'><img class="portrait_image" src='papers/lce/1.png'></div>
		  <img class="portrait_image" src='papers/lce/0.png'>
		</div>
		<script type="text/javascript">
		  function lce_start() {
		      document.getElementById('lce_image').style.opacity = "1";
		  }
		  function lce_stop() {
		      document.getElementById('lce_image').style.opacity = "0";
		  }
		  lce_stop()
		</script>
		<!-- <img class="portrait_image" src='papers/magface/intro1.png'> -->
	      </td>
	      <td valign="top" width="75%">
		<a href="https://irvingmeng.github.io/projects/lce">
		  <papertitle>Learning Compatible Embeddings</papertitle>
		</a>
		<br>
		<strong>Qiang Meng</strong>, 
		Chixiang Zhang, Xiaqiang Xu, Feng Zhou
		<br>
		<em>ICCV</em>, 2021 
		<br>
		<a href="https://irvingmeng.github.io/projects/lce">project page</a>
		| 
		<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Learning_Compatible_Embeddings_ICCV_2021_paper.pdf">paper</a>
		| 
		<a href="https://zhuanlan.zhihu.com/p/475797875">知乎</a>
		| 
		<a href="https://www.bilibili.com/video/BV1au411f7n5/">short video</a>
		|
		<a href="https://github.com/IrvingMeng/LCE">code</a> 
		<iframe src="https://ghbtns.com/github-btn.html?user=IrvingMeng&repo=LCE&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
		<br>
		A general framework (LCE) that is applicable for both cross model compatibility and compatible training in direct, forward, and backward manners. 
	      </td>
	    </tr>
	  </table>

	  <table bgcolor="#ffffd0" cellspacing="10">
	    <tr onmouseout="magface_stop()" onmouseover="magface_start()" >
	      <td width="25%">
		<div class="one">
		  <div class="two" id = 'magface_image'><img class="portrait_image" src='papers/magface/1.png'></div>
		  <img class="portrait_image" src='papers/magface/0.png'>
		</div>
		<script type="text/javascript">
		  function magface_start() {
		      document.getElementById('magface_image').style.opacity = "1";
		  }
		  function magface_stop() {
		      document.getElementById('magface_image').style.opacity = "0";
		  }
		  magface_stop()
		</script>
		<!-- <img class="portrait_image" src='papers/magface/intro1.png'> -->
	      </td>
	      <td valign="top" width="75%">
		<a href="https://irvingmeng.github.io/projects/magface">
		  <papertitle>MagFace: A Universal Representation for Face Recognition and Quality Assessment</papertitle>
		</a>
		<br>
		<strong></strong><strong>Qiang Meng</strong>, 
		Shichao Zhao, Zhida Huang, Feng Zhou
		<!-- <a href="https://www.linkedin.com/in/ganeshjcs">test</a> -->
		<br>
		<em>CVPR</em>, 2021  <strong><font color="#FF0000">(Oral presentation)</font> </strong>
		<br>
		<a href="https://irvingmeng.github.io/projects/magface">project page</a>
		|
		<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Meng_MagFace_A_Universal_Representation_for_Face_Recognition_and_Quality_Assessment_CVPR_2021_paper.pdf">paper</a>
		|
		<a href="https://zhuanlan.zhihu.com/p/475775106">知乎</a>
		| 		
		<a href="https://www.bilibili.com/video/BV1Jq4y1j7ZH">short video</a>
		|
		<a href="https://github.com/IrvingMeng/MagFace">code</a>
		<!-- <iframe src="https://ghbtns.com/github-btn.html?user=IrvingMeng&repo=MagFace&type=fork&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe> -->
		<iframe src="https://ghbtns.com/github-btn.html?user=IrvingMeng&repo=MagFace&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
		<br>
		A novel loss which equips feature magnitudes with the ability to represent face qualities, as well as achieves better performances on face recognition and clustering.
		Remarkably, <em>no additional labels are required!</em>
		<!-- https://zhuanlan.zhihu.com/p/393598428 -->
	      </td>
	    </tr>
	  </table>

	  <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/bbs/1.png'>
	    </td>
	      <td valign="top" width="75%">
		<a href="https://arxiv.org/abs/2201.09308">
		  <papertitle>Basket-based Softmax</papertitle>
		</a>
		<br>
		<strong>Qiang Meng</strong>, Xinqian Gu, Xiaqing Xu, Feng Zhou
		<br>
		<em>arXiv</em>, 2022
		<br>
		<a href="https://arxiv.org/pdf/2201.09308">paper</a>
		<br>
		A simple but effective mining-during-training strategy that enables models to be trained in an end-to-end fashion on multiple datasets.
	      </td>
	    </tr>
	  </table>

	  <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/face_align/0.png'>
	    </td>
	    <td valign="top" width="75%">
	      <a href="https://arxiv.org/abs/2102.05447">
		<papertitle>Searching for Alignment in Face Recognition</papertitle>
	      </a>
	      <br>
	      Xiaqing Xu, <strong></strong><strong>Qiang Meng</strong>,
	      Yunxiao Qin, Jianzhu Guo, Chenxu Zhao, Feng Zhou, Zhen Lei
	      <!-- <a href="https://www.linkedin.com/in/ganeshjcs">test</a> -->
	      <br>
	      <em>AAAI</em>, 2021 
	      <br>
	      <a href="https://cdn.aaai.org/ojs/16415/16415-13-19909-1-2-20210518.pdf">paper</a>
	      |
	      <a href="https://zhuanlan.zhihu.com/p/475785780">知乎</a>

	      <br>      
	      We design a face template searching space with decomposed crop size and vertical shift, and propose the Face Alignment Policy Search (FAPS) to find optimal alignment templates for face recognition. 
	    </td>
	  </table>


	  <table cellspacing="10">
	    <td width="25%">
	      <!-- <div class="one">
		   <div class="two" id = 'poseface_image'><img class="portrait_image" src='papers/poseface/0.png'></div>
		   <img class="portrait_image" src='papers/poseface/1.png'>
	      </div>
	      <script type="text/javascript">
		function poseface_start() {
		document.getElementById('poseface_image').style.opacity = "1";
		}
		function poseface_stop() {
		document.getElementById('poseface_image').style.opacity = "0";
		}
		poseface_stop()
	      </script> -->
	      <img class="portrait_image" src='papers/poseface/1.png'>
	    </td>
	    <td valign="top" width="75%">
	      <a href="https://arxiv.org/abs/2107.11721">
		<papertitle>PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition</papertitle>
	      </a>
	      <br>
	      <strong>Qiang Meng</strong>, Xiaqing Xu, Xiaobo Wang, Yang Qian, Yunxiao Qin, Zezheng Wang, Chenxu Zhao, Feng Zhou, Zhen Lei
	      <br>
	      <em>arXiv</em>, 2021 
		  <br>
		  <a href="https://arxiv.org/pdf/2107.11721">paper</a>
	      <br>
	      An efficient large-pose face recognition method that leverages facial landmarks to disentangle pose-invariant features and incorporates a pose-adaptive loss to dynamically address the imbalance issue.
	    </td>
	  </table>


	  	  <!-- <table cellspacing="10">
	    <td width="25%">
	      <img class="portrait_image" src='papers/chi/1.png'>
	    </td>
	    <td valign="top" width="75%">
	      <a href="https://www.sciencedirect.com/science/article/pii/S1532046417301594">
		<papertitle>CHI: A Contemporaneous Health Index for Degenerative Disease Monitoring using Longitudinal Measurements</papertitle>
	      </a>
	      <br>
	      Yijun Huang, <strong></strong><strong>Qiang Meng</strong>, Heather Evans, William Lober, Yu Cheng, Xiaoning Qian, Ji Liu, Shuai Huang
	      <br>
	      <em> Journal of biomedical informatics</em>, 2017
	      <br>
	      A optimization formulation is developed for contemporaneous patient risk monitoring by exploiting the emerging data-rich environment in healthcare applications.
	    </td>
	  </table> -->


    </table>

    <br>

    <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
      <tr>
	<td>
	  <heading>Honors </heading>
	  <ul>
            <li> <strong>[2015]</strong>  College of Engineering Dean's Fellowship, <em>University of Washington</em></li>
            <li><strong>[2014]</strong>  Samsung Scholarship, <em>University of Science and Technology of China</em></li>
            <li><strong>[2013]</strong>  National Encouragement Scholarship, <em>University of Science and Technology of China</em></li>
            <li><strong>[2013]</strong>  First prize in the Challenge Cup, <em>University of Science and Technology of China</em></li>
            <li><strong>[2012]</strong>  National Encouragement Scholarship, <em>University of Science and Technology of China</em></li>   
            <li><strong>[2012]</strong>  1st place in RoboGame Robot Competition, <em>University of Science and Technology of China</em></li>  	
	  </ul>
	</td>
      </tr>
    </table>

    <br>
    <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
      <tr>
	<td>
	  <heading>Teaching </heading>
	  <ul>
            <li>IND E 315 - <strong>Probability and Statistics for Engineers</strong> [Spring 17] [Summer 17] [Winter 18] [Spring 18]</li> 
            <li>IND E 250 - <strong>Linear and Network Programming</strong> [Fall 16] [Fall 17] </li>
            <li>IND E 410 - <strong>Fundamentals of Engineering Economy</strong> [Winter 16]</li>
	  </ul>
	</td>
      </tr>
    </table>
    <br>



    <center>
      <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=TQza_Ty60YCW_BJcSVuk9C0IflO_Jo6Jz2qoD6Bgju4&co=f9f9f9&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
      <!-- <a href="https://clustrmaps.com/site/1blzm" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=TQza_Ty60YCW_BJcSVuk9C0IflO_Jo6Jz2qoD6Bgju4&cl=ffffff"></a> -->
    </center>


    <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
      <!-- <tr> -->
	<!--   <td> -->
	  <!--     <br> -->
	  <!--     <hr> -->
	  <!-- </tr> -->
      <hr>

      <center>
	<foot>
	  <td width="75%">
	    Website adapted from <u><a href="https://jonbarron.info/">Jon Barron </a></u> & <u><a href="https://github.com/abhoi/abhoi.github.io">Amlaan Bhoi</a></u>
	  </td>
	  <td width="30%">
	    Last updated in April 2024
	  </td>
	</foot>
      </center>
    </table>


  </body>
</html>

